{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Text Summarization using TensorFlow and Attention"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Import necessary libraries\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, LSTM, Dense, Embedding, Bidirectional, Concatenate, Attention\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import tensorflow_datasets as tfds\n",
    "\n",
    "tf.random.set_seed(42)\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Load and Explore Dataset"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "dataset, info = tfds.load('cnn_dailymail', with_info=True, as_supervised=True)\n",
    "train_dataset, val_dataset, test_dataset = dataset['train'], dataset['validation'], dataset['test']\n",
    "\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Data Preprocessing and Tokenization"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Define parameters\n",
    "MAX_ARTICLE_LENGTH = 400\n",
    "MAX_SUMMARY_LENGTH = 100\n",
    "VOCAB_SIZE = 20000\n",
    "\n",
    "article_tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")\n",
    "summary_tokenizer = Tokenizer(num_words=VOCAB_SIZE, oov_token=\"<OOV>\")"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Preparing a subset of data for tokenizers\n",
    "max_samples = 50000\n",
    "articles, summaries = [], []\n",
    "\n",
    "for i, (article, summary) in enumerate(train_dataset):\n",
    "    if i >= max_samples:\n",
    "        break\n",
    "    articles.append(article.numpy().decode())\n",
    "    summaries.append('<START> ' + summary.numpy().decode() + ' <END>')\n",
    "\n",
    "article_tokenizer.fit_on_texts(articles)\n",
    "summary_tokenizer.fit_on_texts(summaries)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Build and Compile the Model"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "def build_model(vocab_size, embedding_dim=128, lstm_units=256):\n",
    "    encoder_inputs = Input(shape=(MAX_ARTICLE_LENGTH,))\n",
    "    encoder_embedding = Embedding(vocab_size, embedding_dim)(encoder_inputs)\n",
    "    encoder_lstm = Bidirectional(LSTM(lstm_units, return_sequences=True, return_state=True))\n",
    "    encoder_outputs, forward_h, forward_c, backward_h, backward_c = encoder_lstm(encoder_embedding)\n",
    "    \n",
    "    state_h = Concatenate()([forward_h, backward_h])\n",
    "    state_c = Concatenate()([forward_c, backward_c])\n",
    "\n",
    "    decoder_inputs = Input(shape=(MAX_SUMMARY_LENGTH-1,))\n",
    "    decoder_embedding = Embedding(vocab_size, embedding_dim)(decoder_inputs)\n",
    "    decoder_lstm = LSTM(lstm_units*2, return_sequences=True, return_state=True)\n",
    "    decoder_outputs, _, _ = decoder_lstm(decoder_embedding, initial_state=[state_h, state_c])\n",
    "\n",
    "    attention = Attention()([decoder_outputs, encoder_outputs])\n",
    "    decoder_combined = Concatenate()([decoder_outputs, attention])\n",
    "\n",
    "    outputs = Dense(vocab_size, activation='softmax')(decoder_combined)\n",
    "    model = Model([encoder_inputs, decoder_inputs], outputs)\n",
    "    return model\n",
    "\n",
    "model = build_model(VOCAB_SIZE)\n",
    "model.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Training and validation logic here (as provided in your script)"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluate and Test the Model"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "# Evaluation, inference logic and plotting history as provided in your script"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Save Tokenizers"
   ]
  },
  {
   "cell_type": "code",
   "source": [
    "import pickle\n",
    "with open('article_tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(article_tokenizer, handle)\n",
    "\n",
    "with open('summary_tokenizer.pickle', 'wb') as handle:\n",
    "    pickle.dump(summary_tokenizer, handle)"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
